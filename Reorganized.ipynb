{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries you might not have\n",
    "# !python3 -m pip install --upgrade nbconvert \n",
    "# !python3 -m pip install --upgrade nbstripout \n",
    "# !python3 -m pip install tomotopy\n",
    "# !python3 -m pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Things to install from nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tomotopy as tp\n",
    "from itertools import chain\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os.path\n",
    "\n",
    "import dataloader\n",
    "import bow\n",
    "import slda\n",
    "import post_classifier\n",
    "import aggregate\n",
    "import user_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(POSTPATH, LABELPATH, USERPATH, FOLDERPATH, subset = 1000, append_title = False, filter_images=True):\n",
    "    print('START: Processing data')\n",
    "    users = dataloader.load_user_subset_from_train(USERPATH, subset = subset)\n",
    "    \n",
    "    user_to_post, post_to_words, post_to_metadata = dataloader.load_posts(POSTPATH, user_subset = users, append_title = append_title)\n",
    "    post_to_label = dataloader.load_classification(LABELPATH, user_to_post, post_to_words, post_to_metadata, user_subset = users)\n",
    "    filtered_data, sw_posts, sw_timestamps = dataloader.filter_posts(post_to_label, post_to_metadata, filter_images=filter_images)\n",
    "    print(len(filtered_data))\n",
    "    filtered_data = dataloader.filter_near_SW(filtered_data,post_to_metadata, sw_timestamps)\n",
    "    print(len(filtered_data))\n",
    "\n",
    "    filtered_data = dataloader.filter_stopwords(filtered_data)\n",
    "    sw_posts = dataloader.filter_stopwords(sw_posts)\n",
    "    \n",
    "    dataloader.save_to_folder(FOLDERPATH, user_to_post, post_to_metadata, filtered_data, sw_posts, sw_timestamps)\n",
    "    print('DONE: Processing data')\n",
    "    return user_to_post, post_to_metadata, filtered_data, sw_posts, sw_timestamps\n",
    "\n",
    "def load_processed_data(FOLDERPATH):\n",
    "    print('IN PROGRESS: Loading data')\n",
    "    return dataloader.load_from_folder(FOLDERPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_label_size(data):\n",
    "    a_len = len([data[key] for key in data.keys() if data[key][2] == 'a'])\n",
    "    b_len = len([data[key] for key in data.keys() if data[key][2] == 'b'])\n",
    "    c_len = len([data[key] for key in data.keys() if data[key][2] == 'c'])\n",
    "    d_len = len([data[key] for key in data.keys() if data[key][2] == 'd'])\n",
    "    total_len = len(data)\n",
    "    control_len = total_len - a_len - b_len - c_len - d_len\n",
    "    print('a: ', a_len)\n",
    "    print('b: ', b_len)\n",
    "    print('c: ', c_len)\n",
    "    print('d: ', d_len)\n",
    "    print('controls: ', control_len)\n",
    "    print('total: ', total_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_feature_model(data, data_type='filter_normal', sLDA=True, BOW=True, num_topics=40):\n",
    "    if sLDA:\n",
    "        slda_model_filename = 'slda_'+data_type+'_'+str(num_topics)+'_model.bin'\n",
    "        slda_vectors_filename = 'slda_'+data_type+'_'+str(num_topics)+'_vectors.pickle'\n",
    "\n",
    "        if os.path.exists(slda_model_filename):\n",
    "            print('IN PROGRESS: Loading sLDA model')\n",
    "            slda_model = tp.SLDAModel.load(slda_model_filename)\n",
    "        else:\n",
    "            print('START: Training sLDA model')\n",
    "            slda_model = slda.train_slda_model_from_data(data, topics=num_topics)\n",
    "            slda_model.save(slda_model_filename)\n",
    "            print('IN PROGRESS: Saving sLDA model')\n",
    "        # print out topics and top n words\n",
    "        get_topics(slda_model)\n",
    "        \n",
    "        slda_vectors = extract_slda_features(data, slda_model, slda_vectors_filename, test=False)\n",
    "    else:\n",
    "        slda_model = 'None'\n",
    "        slda_vectors = 'None'\n",
    "        \n",
    "    if BOW:\n",
    "        pca_model_filename = 'train_pca_'+data_type+'_'+'model.pickle'\n",
    "        bow_vectors_filename = 'train_bow_'+data_type+'_'+'vectors.pickle'\n",
    "        word2index_filename = 'train_bow_word_word2index.pickle'\n",
    "        index2word_filename = 'train_bow_word_index2word.pickle'\n",
    "        \n",
    "        if os.path.exists(pca_model_filename) and os.path.exists(bow_vectors_filename):\n",
    "            print('IN PROGRESS: Loading PCA model')\n",
    "            with open(pca_model_filename, 'rb') as f:\n",
    "                pca_model = pickle.load(f)\n",
    "            with open(bow_vectors_filename, 'rb') as f:\n",
    "                bow_vectors = pickle.load(f)\n",
    "            with open(word2index_filename, 'rb') as f:\n",
    "                word2index = pickle.load(f)\n",
    "            with open(index2word_filename, 'rb') as f:\n",
    "                index2word = pickle.load(f)\n",
    "        else:\n",
    "            print('START: Training BOW model')\n",
    "            word2index, index2word = bow.generate_vocabulary(data)\n",
    "            pca_model, bow_vectors = bow.get_PCA_vectors_from_post_set(data, word2index)\n",
    "            print('IN PROGRESS: Saving BOW model')\n",
    "            with open(pca_model_filename, 'wb') as f:\n",
    "                pickle.dump(pca_model, f)\n",
    "            with open(bow_vectors_filename, 'wb') as f:\n",
    "                pickle.dump(bow_vectors, f)\n",
    "            with open(word2index_filename, 'wb') as f:\n",
    "                pickle.dump(word2index, f)\n",
    "            with open(index2word_filename, 'wb') as f:\n",
    "                pickle.dump(index2word, f)\n",
    "    else:\n",
    "        pca_model = 'None'\n",
    "        bow_vectors = 'None'\n",
    "        word2index = 'None'\n",
    "        index2word = 'None'\n",
    "        \n",
    "    return slda_model, slda_vectors, pca_model, bow_vectors, word2index, index2word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_slda_features(data, slda_model, slda_vectors_filename, test=False):\n",
    "    if test:\n",
    "        slda_vectors_filename = 'test_'+slda_vectors_filename\n",
    "    else:\n",
    "        slda_vectors_filename = 'train_'+slda_vectors_filename\n",
    "    \n",
    "    if os.path.exists(slda_vectors_filename):\n",
    "        with open(slda_vectors_filename, 'rb') as f:\n",
    "            print('IN PROGRESS: Loading sLDA vectors')\n",
    "            slda_vectors=pickle.load(f)\n",
    "    else:\n",
    "        print('START: Getting topic sLDA vecs')\n",
    "        slda_vectors = slda.get_topic_vecs(slda_model, data)\n",
    "        print('IN PROGRESS: Saving sLDA vectors')\n",
    "        with open(slda_vectors_filename, 'wb') as f:\n",
    "            pickle.dump(slda_vectors, f)\n",
    "    return slda_vectors\n",
    "\n",
    "def extract_bow_features(data, word2index, pca_model):\n",
    "    _, bow_vectors = bow.get_PCA_vectors_from_post_set(data, word2index, pca_model=pca_model)\n",
    "    \n",
    "    return bow_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(model):\n",
    "    slda_coefficients = model.get_regression_coef(0)\n",
    "    data = []\n",
    "    for k in range(model.k):\n",
    "        top_words = model.get_topic_words(k, top_n=40)\n",
    "        words = [word for (word, float) in top_words]\n",
    "        words = \", \".join(words)\n",
    "        data.append([words, slda_coefficients[k]])\n",
    "\n",
    "    indices = np.array(slda_coefficients).argsort()\n",
    "    data = np.array(data)\n",
    "    data = data[indices]\n",
    "\n",
    "    topics = pd.DataFrame(data, columns=[\"Topic\", \"Suicidality Coefficient\"])\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slda_vectorize_data(model, FOLDERPATH):\n",
    "    X, y, post_vectors = slda.vectorize_data_set(model, FOLDERPATH)\n",
    "    return  X, y, post_vectors\n",
    "    \n",
    "def minmax_norm(arr):    \n",
    "    return (arr - np.min(arr))/(np.max(arr) -np.min(arr))\n",
    "\n",
    "def format_features(slda_model, slda_vectors, pca_model, bow_vectors, FOLDERPATH, num_topics=0,\n",
    "                    data_type='filter_normal', sLDA=True, BOW=True):\n",
    "    post_vectors = ''\n",
    "    X_file = 'X_'+data_type+'_sLDA='+str(sLDA)+'_'+str(num_topics)+'_BOW='+str(BOW)+'.pickle'\n",
    "    y_file = 'y_'+data_type+'_sLDA='+str(sLDA)+'_'+str(num_topics)+'_BOW='+str(BOW)+'.pickle'\n",
    "    post_vectors_file = 'post_vectors_'+data_type+'_sLDA='+str(sLDA)+'_'+str(num_topics)+'_BOW='+str(BOW)+'.pickle'\n",
    "    \n",
    "    if os.path.exists(X_file) and os.path.exists(y_file) and os.path.exists(post_vectors_file):\n",
    "        with open(X_file,'rb') as f:\n",
    "            X = pickle.load(f)\n",
    "        with open(y_file,'rb') as f:\n",
    "            y = pickle.load(f)\n",
    "        with open(post_vectors_file,'rb') as f:\n",
    "            post_vectors = pickle.load(f)\n",
    "    else:\n",
    "        if sLDA and BOW:\n",
    "            print('START: Vectorize sLDA and BOW')\n",
    "            X = np.array([np.concatenate([minmax_norm(slda_vectors[key][0]),minmax_norm(bow_vectors[key][0])]) for key in slda_vectors.keys()])\n",
    "            y = np.array([slda_vectors[key][1] for key in slda_vectors.keys()])\n",
    "            y = y.reshape(np.shape(y)[0])\n",
    "        elif sLDA:\n",
    "            print('START: Vectorize sLDA')\n",
    "            X, y, post_vectors = slda_vectorize_data(slda_model, FOLDERPATH)\n",
    "            y = y.reshape(np.shape(y)[0])\n",
    "        elif BOW:\n",
    "            print('START: Vectorize BOW')\n",
    "            X = np.array([bow_vectors[key][0] for key in bow_vectors.keys()])\n",
    "            y = np.array([bow_vectors[key][1] for key in bow_vectors.keys()])\n",
    "            y = y.reshape(np.shape(y)[0])\n",
    "        print('DONE: Vectorizing')\n",
    "        print('IN PROGRESS: Saving vectors')\n",
    "        with open(X_file,'wb') as f:\n",
    "            pickle.dump(X, f)\n",
    "        with open(y_file,'wb') as f:\n",
    "            pickle.dump(y, f)\n",
    "        with open(post_vectors_file,'wb') as f:\n",
    "            pickle.dump(post_vectors, f)\n",
    "    return X, y, post_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-level Classification Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_post_classifier(X_train, y_train, post_clf_type='RbfSVM', num_topics=40, \n",
    "                          data_type='filter_normal', sLDA=False, BOW=False):\n",
    "    file = 'p_clf_'+data_type+'_sLDA='+str(sLDA)+'_'+str(num_topics)+'_BOW='+str(BOW)+'_'+post_clf_type+'.pickle'\n",
    "    if os.path.exists(file):\n",
    "        print('IN PROGRESS: Loading post classifier')\n",
    "        with open(file, 'rb') as f:\n",
    "            p_clf = pickle.load(f)\n",
    "    else:\n",
    "        print('START: Training post classifier')\n",
    "#         post_clf_types = ['LogReg', 'LinearSVM', 'RbfSVM', 'AdaBoost', 'RandomForest', 'MLP']\n",
    "#         if post_clf_types[0] == post_clf_type:\n",
    "            #param_dict = {'C':[0.2,0.5,0.7,1,1.5,2,5]}\n",
    "#         elif post_clf_types[1] == post_clf_type:\n",
    "            #param_dict = {'C':[0.2,0.5,1,2]}\n",
    "        #p_clf = post_classifier.PostClassification(post_clf_type)\n",
    "#         elif post_clf_types[2] == post_clf_type:\n",
    "            #param_dict = {'C':[0.5,1,2,5]}           \n",
    "#         elif post_clf_types[5] == post_clf_type:\n",
    "            #param_dict = {'hidden_layer_sizes':[(64,64),(64,64,64),(32,32), (32,32,32)], 'learning_rat\n",
    "        #p_clf.train_grid_search_CV(X_train, y_train, param_dict, groups=5)\n",
    "\n",
    "        \n",
    "        p_clf = post_classifier.PostClassification(post_clf_type)\n",
    "        p_clf.train(X_train, y_train)\n",
    "        print('IN PROGRESS: Saving post classifier')\n",
    "        with open(file, 'wb') as f:\n",
    "            pickle.dump(p_clf, f)\n",
    "    return p_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-level Classification Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_posts(p_clf, X_test, y_test, print_metrics=True):\n",
    "    y_pred = p_clf.test(X_test)\n",
    "    if print_metrics:\n",
    "        p_clf.get_metrics(y_test, y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-level Classifcation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_post_labels(data, post_vectors, y_pred, FOLDERPATH, sLDA=True, BOW=False):\n",
    "    # relabel the user labels from 1 for d and 0 for all else\n",
    "    user_to_y = defaultdict(int)\n",
    "    for post_id in tqdm.tqdm(data.keys()):\n",
    "        user_to_y[data[post_id][0]] = (1 if data[post_id][2] == 'd' else 0)\n",
    "    \n",
    "    # format the data to get user for each post\n",
    "    post2user_ypred = defaultdict(list)\n",
    "    for i, post_id in enumerate(post_vectors.keys()):\n",
    "        user_id = data[post_id][0]\n",
    "        post2user_ypred[post_id] = [user_id, y_pred[i]]\n",
    "        \n",
    "    user_to_post_label = aggregate.aggregate_posts(FOLDERPATH, post2user_ypred)\n",
    "    \n",
    "    return user_to_post_label, user_to_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_user_pred(user_to_y, user_to_ypred):\n",
    "    user_y = []\n",
    "    user_y_pred = []\n",
    "    for user_id in user_to_ypred:\n",
    "        user_y.append(user_to_y[user_id])\n",
    "        user_y_pred.append(user_to_ypred[user_id])\n",
    "    return user_y, user_y_pred\n",
    "\n",
    "def predict_users(user_to_post_label, user_to_y, user_clf_type='Max'):\n",
    "    u_clf = user_classifier.UserClassification(user_to_post_label)\n",
    "    if user_clf_type=='Max':\n",
    "        user_to_ypred = u_clf.argmax()\n",
    "    elif user_clf_type=='Threshold':\n",
    "        user_to_ypred = u_clf.find_threshold(user_to_y)\n",
    "    elif user_clf_type=='Minimum':\n",
    "        user_to_ypred = u_clf.minimum(1)\n",
    "    \n",
    "    user_y, user_ypred = format_user_pred(user_to_y, user_to_ypred)\n",
    "    u_clf.get_metrics(user_y, user_ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapped(train_data_processed=True, dev_data_processed=True, sLDA=True, \n",
    "            data_type='filter_normal', num_topics=40, BOW=True, post_clf_type='RbfSVM',\n",
    "            user_clf_types=['Max']):\n",
    "    # Make sure you have created TRAIN_FOLDERPATH and DEV_FOLDERPATH directories on local\n",
    "    TRAIN_POSTPATH = './Data/crowd/train/shared_task_posts.csv'\n",
    "    TRAIN_LABELPATH = './Data/crowd/train/crowd_train.csv'\n",
    "    TRAIN_USERPATH = './Data/crowd/train/task_C_train.posts.csv'\n",
    "    \n",
    "    DEV_POSTPATH = './Data/crowd/test/shared_task_posts_test.csv'\n",
    "    DEV_LABELPATH = './Data/crowd/test/crowd_test_C.csv'\n",
    "    DEV_USERPATH = './Data/crowd/test/task_C_test.posts.csv'\n",
    "    \n",
    "    TRAIN_FOLDERPATH = './Processing/crowd_processed/'\n",
    "    DEV_FOLDERPATH = './Processing/crowd_processed_test/'\n",
    "    \n",
    "    # Proccess or load processed training data\n",
    "    if not train_data_processed:\n",
    "        user_to_post, post_to_metadata, filtered_data, sw_posts, sw_timestamps = process_data(TRAIN_POSTPATH, TRAIN_LABELPATH, \n",
    "                                                                                              TRAIN_USERPATH, TRAIN_FOLDERPATH)\n",
    "    else:\n",
    "        user_to_post, post_to_metadata, filtered_data, sw_posts, sw_timestamps = load_processed_data(TRAIN_FOLDERPATH)\n",
    "    \n",
    "    # print class sizes\n",
    "    print_label_size(filtered_data)\n",
    "    slda_model, slda_vectors, pca_model, bow_vectors, word2index, index2word  = train_feature_model(filtered_data, data_type=data_type, \n",
    "                                                                                                    sLDA=sLDA, BOW=BOW, num_topics=num_topics)\n",
    "    \n",
    "    X_train, y_train, post_vectors = format_features(slda_model, slda_vectors, pca_model, \n",
    "                                                     bow_vectors, TRAIN_FOLDERPATH, sLDA=sLDA, BOW=BOW)\n",
    "    y_train = y_train.reshape(np.shape(y_train)[0])\n",
    "    \n",
    "    p_clf = train_post_classifier(X_train, y_train, post_clf_type=post_clf_type, num_topics=num_topics, \n",
    "                          data_type=data_type, sLDA=sLDA, BOW=BOW)\n",
    "    \n",
    "    print('DONE: Training complete')\n",
    "    \n",
    "    \n",
    "#     Proccess or load processed development data\n",
    "    if not dev_data_processed:\n",
    "        user_to_post, post_to_metadata, filtered_data, sw_posts, sw_timestamps = process_data(DEV_POSTPATH, \n",
    "                                                                                              DEV_LABELPATH, DEV_USERPATH, DEV_FOLDERPATH)\n",
    "    else:\n",
    "        user_to_post, post_to_metadata, filtered_data, sw_posts, sw_timestamps = load_processed_data(DEV_FOLDERPATH)\n",
    "    \n",
    "    print_label_size(filtered_data)\n",
    "    \n",
    "    if sLDA:\n",
    "        slda_vectors_filename = 'slda_'+data_type+'_'+str(num_topics)+'_vectors.pickle'\n",
    "        slda_vectors = extract_slda_features(filtered_data, slda_model, slda_vectors_filename, test=True)\n",
    "    if BOW:\n",
    "        bow_vectors= extract_bow_features(filtered_data, word2index, pca_model)\n",
    "        \n",
    "    X_test, y_test, post_vectors = format_features(slda_model, slda_vectors, pca_model, bow_vectors, DEV_FOLDERPATH, data_type=data_type,\n",
    "                                                   sLDA=sLDA, BOW=BOW, num_topics=num_topics)\n",
    "    \n",
    "    y_pred = predict_posts(p_clf, X_test, y_test, print_metrics=True)\n",
    "    \n",
    "    user_to_post_label, user_to_y = format_post_labels(filtered_data, slda_vectors, y_pred, DEV_FOLDERPATH)\n",
    "    \n",
    "    for user_clf_type in user_clf_types:\n",
    "        print('START: User classification post:', user_clf_type)\n",
    "        predict_users(user_to_post_label, user_to_y, user_clf_type=user_clf_type)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_clf_typs = ['LogReg', 'LinearSVM', 'RbfSVM', 'AdaBoost', 'RandomForest', 'MLP']\n",
    "user_clf_types = ['Max', 'Threshold']\n",
    "wrapped(train_data_processed=True, BOW=True, post_clf_type=post_clf_types[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
