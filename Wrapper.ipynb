{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries you might not have\n",
    "# !python3 -m pip install --upgrade nbconvert \n",
    "# !python3 -m pip install --upgrade nbstripout \n",
    "# !python3 -m pip install tomotopy\n",
    "# !python3 -m pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Things to install from nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tomotopy as tp\n",
    "from itertools import chain\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import sklearn.preprocessing\n",
    "import imblearn.over_sampling\n",
    "\n",
    "import dataloader\n",
    "import bow\n",
    "import slda\n",
    "import post_classifier\n",
    "import aggregate\n",
    "import user_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below to process data if you have not already\n",
    "\n",
    "POSTPATH = './Data/crowd/train/shared_task_posts.csv'\n",
    "LABELPATH = './Data/crowd/train/crowd_train.csv'\n",
    "USERPATH = './Data/crowd/train/task_C_train.posts.csv'\n",
    "\n",
    "users = dataloader.load_user_subset_from_train(USERPATH, subset = 1000)\n",
    "    \n",
    "user_to_post, post_to_words, post_to_metadata = dataloader.load_posts(POSTPATH, user_subset = users, append_title = True)\n",
    "post_to_label = dataloader.load_classification(LABELPATH, user_to_post, post_to_words, post_to_metadata, user_subset = users)\n",
    "filtered_data, sw_posts, sw_timestamps = dataloader.filter_posts(post_to_label, post_to_metadata, filter_images=True)\n",
    "print(len(filtered_data))\n",
    "filtered_data = dataloader.filter_near_SW(filtered_data,post_to_metadata, sw_timestamps)\n",
    "print(len(filtered_data))\n",
    "\n",
    "filtered_data = dataloader.filter_stopwords(filtered_data)\n",
    "sw_posts = dataloader.filter_stopwords(sw_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDERPATH = './Processing/crowd_processed/'\n",
    "dataloader.save_to_folder(FOLDERPATH, user_to_post, post_to_metadata, filtered_data, sw_posts, sw_timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Process Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDERPATH = './Processing/crowd_processed/'\n",
    "user_to_post, post_to_metadata, filtered_data, sw_posts, sw_timestamps = dataloader.load_from_folder(FOLDERPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len([filtered_data[key] for key in filtered_data.keys() if filtered_data[key][2] == 'a']))\n",
    "print(len([filtered_data[key] for key in filtered_data.keys() if filtered_data[key][2] == 'b']))\n",
    "print(len([filtered_data[key] for key in filtered_data.keys() if filtered_data[key][2] == 'c']))\n",
    "print(len([filtered_data[key] for key in filtered_data.keys() if filtered_data[key][2] == 'd']))\n",
    "print(len(filtered_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLDA Model: Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = slda.train_slda_model_from_data(filtered_data, topics=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slda_coefficients = model.get_regression_coef(0)\n",
    "data = []\n",
    "for k in range(model.k):\n",
    "    top_words = model.get_topic_words(k, top_n=40)\n",
    "    words = [word for (word, float) in top_words]\n",
    "    words = \", \".join(words)\n",
    "    data.append([words, slda_coefficients[k]])\n",
    "    \n",
    "indices = np.array(slda_coefficients).argsort()\n",
    "data = np.array(data)\n",
    "data = data[indices]\n",
    "\n",
    "pd.DataFrame(data, columns=[\"Topic\", \"Suicidality Coefficient\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to print example of overly negative topic\n",
    "print(data[np.shape(data)[0]-1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sLDA Features: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_train = slda.get_topic_vecs(model, filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment to print example feature vector\n",
    "print(vector_train['hw4uh'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index,index2word = bow.generate_vocabulary(filtered_data)\n",
    "pca_model, vector_train_bow = bow.get_PCA_vectors_from_post_set(filtered_data, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector_train_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Classifier: Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Classifier: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_norm(arr):\n",
    "    \n",
    "    return (arr - np.min(arr))/(np.max(arr) -np.min(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE JUST SLDA\n",
    "X_train = np.array([ vector_train[key][0] for key in vector_train.keys()])\n",
    "y_train = np.array([ vector_train[key][1] for key in vector_train.keys()])\n",
    "y_train = y_train.reshape(np.shape(y_train)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE JUST BOW\n",
    "X_train = np.array([ vector_train_bow[key][0] for key in vector_train_bow.keys()])\n",
    "y_train = np.array([ vector_train_bow[key][1] for key in vector_train_bow.keys()])\n",
    "y_train = y_train.reshape(np.shape(y_train)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE SLDA + BOW\n",
    "X_train = np.array([ np.concatenate([minmax_norm(vector_train[key][0]),minmax_norm(vector_train_bow[key][0])]) for key in vector_train.keys()])\n",
    "y_train = np.array([ vector_train[key][1] for key in vector_train.keys()])\n",
    "\n",
    "y_train = y_train.reshape(np.shape(y_train)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = imblearn.over_sampling.RandomOverSampler(random_state=0)\n",
    "X_train, y_train = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNCOMMENT TO RUN GRID SEARCH CV\n",
    "#p_clf = post_classifier.PostClassification(\"LogReg\")\n",
    "#param_dict = {'C':[0.2,0.5,0.7,1,1.5,2,5]}\n",
    "#p_clf.train_grid_search_CV(X_train, y_train, param_dict, groups=5)\n",
    "\n",
    "#RUN WITH OPTIMAL PARAMETERS\n",
    "p_clf = post_classifier.PostClassification(\"LogReg\")\n",
    "p_clf.train(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNCOMMENT TO RUN GRID SEARCH CV\n",
    "#p_clf = post_classifier.PostClassification(\"LinearSVM\")\n",
    "#param_dict = {'C':[0.2,0.5,1,2]}\n",
    "#p_clf.train_grid_search_CV(X_train, y_train, param_dict, groups=5)\n",
    "\n",
    "p_clf = post_classifier.PostClassification(\"LinearSVM\")\n",
    "p_clf.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNCOMMENT TO RUN GRID SEARCH CV\n",
    "#p_clf = post_classifier.PostClassification(\"RbfSVM\")\n",
    "#param_dict = {'C':[0.5,1,2,5]}\n",
    "#p_clf.train_grid_search_CV(X_train, y_train, param_dict, groups=5)\n",
    "\n",
    "p_clf = post_classifier.PostClassification(\"RbfSVM\")\n",
    "p_clf.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_clf = post_classifier.PostClassification(\"AdaBoost\")\n",
    "p_clf.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_clf = post_classifier.PostClassification(\"RandomForest\")\n",
    "p_clf.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNCOMMENT TO RUN GRID SEARCH CV\n",
    "#p_clf = post_classifier.PostClassification(\"MLP\")\n",
    "#param_dict = {'hidden_layer_sizes':[(64,64),(64,64,64),(32,32), (32,32,32)], 'learning_rate': ('constant', 'adaptive')}\n",
    "#p_clf.train_grid_search_CV(X_train, y_train, param_dict, groups=5)\n",
    "\n",
    "p_clf = post_classifier.PostClassification(\"MLP\")\n",
    "p_clf.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Post Classifier: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = p_clf.test(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_clf.get_metrics(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(y_pred_train))\n",
    "print(sum(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Classfier: Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change y from a, b, c, d, control to -, 1\n",
    "user_to_y_train = defaultdict(int)\n",
    "for data in tqdm.tqdm(filtered_data.keys()):\n",
    "    user_to_y_train[filtered_data[data][0]] = (1 if filtered_data[data][2] == 'd' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_to_uypred_train = defaultdict(list)\n",
    "\n",
    "for i, post_id in enumerate(vector_train.keys()):\n",
    "    user_id = filtered_data[post_id][0]\n",
    "    post_to_uypred_train[post_id] = [user_id, y_pred_train[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_to_post_label_train = aggregate.aggregate_posts(FOLDERPATH, post_to_uypred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argmax: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_clf_train = user_classifier.UserClassification(user_to_post_label_train)\n",
    "user_to_ypred_train = u_clf_train.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_y_train = []\n",
    "user_y_pred_train = []\n",
    "for user_id in user_to_ypred_train:\n",
    "    user_y_train.append(user_to_y_train[user_id])\n",
    "    user_y_pred_train.append(user_to_ypred_train[user_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_clf_train.get_metrics(user_y_train, user_y_pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data: Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSTPATH2 = './Data/crowd/test/shared_task_posts_test.csv'\n",
    "LABELPATH2 = './Data/crowd/test/crowd_test_C.csv'\n",
    "USERPATH2 = './Data/crowd/test/task_C_test.posts.csv'\n",
    "    \n",
    "user_to_post_test, post_to_words_test, post_to_metadata_test = dataloader.load_posts(POSTPATH2, append_title = True)\n",
    "post_to_label_test = dataloader.load_classification(LABELPATH2, user_to_post_test, post_to_words_test, post_to_metadata_test)\n",
    "filtered_data_test, sw_posts_test, sw_timestamps_test = dataloader.filter_posts(post_to_label_test, post_to_metadata_test, filter_images = True)\n",
    "print(len(filtered_data_test))\n",
    "filtered_data_test = dataloader.filter_near_SW(filtered_data_test, post_to_metadata_test, sw_timestamps_test)\n",
    "print(len(filtered_data_test))\n",
    "\n",
    "filtered_data_test = dataloader.filter_stopwords(filtered_data_test)\n",
    "sw_posts_test = dataloader.filter_stopwords(sw_posts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDERPATH2 = './Processing/crowd_processed_test/'\n",
    "dataloader.save_to_folder(FOLDERPATH2, user_to_post_test, post_to_metadata_test, filtered_data_test, sw_posts_test, sw_timestamps_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Process Data: Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDERPATH2 = './Processing/crowd_processed_test/'\n",
    "user_to_post_test, post_to_metadata_test, filtered_data_test, sw_posts_test, sw_timestamps_test = dataloader.load_from_folder(FOLDERPATH2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len([filtered_data_test[key] for key in filtered_data_test.keys() if filtered_data_test[key][2] == 'a']))\n",
    "print(len([filtered_data_test[key] for key in filtered_data_test.keys() if filtered_data_test[key][2] == 'b']))\n",
    "print(len([filtered_data_test[key] for key in filtered_data_test.keys() if filtered_data_test[key][2] == 'c']))\n",
    "print(len([filtered_data_test[key] for key in filtered_data_test.keys() if filtered_data_test[key][2] == 'd']))\n",
    "print(len(filtered_data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction: Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = slda.vectorize_data_set(model, FOLDERPATH2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_test = slda.get_topic_vecs(model, filtered_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, vector_test_bow = bow.get_PCA_vectors_from_post_set(filtered_data_test, word2index, pca_model=pca_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_test_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE JUST BOW\n",
    "X_test = np.array([ vector_test_bow[key][0] for key in vector_test_bow.keys()])\n",
    "y_test = np.array([ vector_test_bow[key][1] for key in vector_test_bow.keys()])\n",
    "y_test = y_test.reshape(np.shape(y_test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE SLDA + BOW\n",
    "X_test = np.array([ np.concatenate([minmax_norm(vector_test[key][0]),minmax_norm(vector_test_bow[key][0])]) for key in vector_test.keys()])\n",
    "y_test = np.array([ vector_test[key][1] for key in vector_test.keys()])\n",
    "\n",
    "y_test = y_test.reshape(np.shape(y_test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(X_test))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Classifier: Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Post Classifier: Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = p_clf.test(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_clf.get_metrics(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(y_pred_test))\n",
    "print(sum(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Classifier: Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate: Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change y from a, b, c, d, control to -, 1\n",
    "user_to_y_test = defaultdict(int)\n",
    "for data in tqdm.tqdm(filtered_data_test.keys()):\n",
    "    user_to_y_test[filtered_data_test[data][0]] = (1 if filtered_data_test[data][2] == 'd' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vector_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_to_uypred_test = defaultdict(list)\n",
    "\n",
    "for i, post_id in enumerate(vector_test.keys()):\n",
    "    user_id = filtered_data_test[post_id][0]\n",
    "    post_to_uypred_test[post_id] = [user_id, y_pred_test[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_to_post_label_test = aggregate.aggregate_posts(FOLDERPATH2, post_to_uypred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argmax: Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_clf_test = user_classifier.UserClassification(user_to_post_label_test)\n",
    "user_to_ypred_test = u_clf_test.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_y_test = []\n",
    "user_y_pred_test = []\n",
    "for user_id in user_to_ypred_test:\n",
    "    user_y_test.append(user_to_y_test[user_id])\n",
    "    user_y_pred_test.append(user_to_ypred_test[user_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_clf_test.get_metrics(user_y_test, user_y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold: Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_clf_test = user_classifier.UserClassification(user_to_post_label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_clf_test.find_threshold(user_to_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_to_ypred_test = u_clf_test.minimum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_y_test = []\n",
    "user_y_pred_test = []\n",
    "for user_id in user_to_ypred_test:\n",
    "    user_y_test.append(user_to_y_test[user_id])\n",
    "    user_y_pred_test.append(user_to_ypred_test[user_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_clf_test.get_metrics(user_y_test, user_y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
